{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Training with `mister_ed`\n",
    "This file will contain the basics on how to perform adversarial training under the `mister_ed` framework. It's highly recommended that you have walked through tutorial_1 before going through this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll start by importing everything we'll need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTERNAL LIBRARY IMPORTS\n",
    "import numpy as np \n",
    "import scipy \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch # Need torch version >=0.3\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "assert float(torch.__version__[:3]) >= 0.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MISTER ED SPECIFIC IMPORT BLOCK\n",
    "# (here we do things so relative imports work )\n",
    "# Universal import block \n",
    "# Block to get the relative imports working \n",
    "import os\n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import config\n",
    "import prebuilt_loss_functions as plf\n",
    "import loss_functions as lf \n",
    "import utils.pytorch_utils as utils\n",
    "import utils.image_utils as img_utils\n",
    "import cifar10.cifar_loader as cifar_loader\n",
    "import cifar10.cifar_resnets as cifar_resnets\n",
    "import adversarial_training as advtrain\n",
    "import adversarial_evaluation as adveval\n",
    "import utils.checkpoints as checkpoints\n",
    "import adversarial_perturbations as ap \n",
    "import adversarial_attacks as aa\n",
    "import spatial_transformers as st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define what we want to do here:\n",
    "\n",
    "Our goal is to run through a few training epochs of a pretrained classifier where we augment the training data with a set of adversarial examples. For simplicity's sake, let's just try and train a few epochs of a 20-layer ResNet trained on CIFAR-10, defended against an FGSM attack of $\\epsilon=8$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's instatiate our pretrained classifier and our training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_trainset = cifar_loader.load_cifar_data('train')\n",
    "model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=20, return_normalizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's build the attack parameters: an object that contains all the information to perform an attack on a minibatch. So first let's build an attack object and then furnish it with the necessary kwargs. \n",
    "\n",
    "Like in tutorial 1, to create an attack object, we'll need to create a threat model and a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, \n",
    "                              {'lp_style': 'inf', \n",
    "                               'lp_bound': 8.0 / 255})\n",
    "attack_loss = plf.VanillaXentropy(model, normalizer)\n",
    "attack_object = aa.FGSM(model, normalizer, delta_threat, attack_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we build the `AttackParameters` object, which just wraps the attack object with the kwargs needed to call the `attack(...)` method on attack. For FGSM attacks, we just want to turn the verbosity off, but for more complicated attacks, this will be more involved. Typically in training, we generate a single adversarial example per training point, but to be speedy here, let's only create 1 example per every 5 training points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_kwargs = {'verbose': False} # kwargs to be called in attack_object.attack(...)\n",
    "attack_params = advtrain.AdversarialAttackParameters(attack_object, proportion_attacked=0.2, \n",
    "                                                     attack_specific_params={'attack_kwargs': attack_kwargs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our attack parameters built, we can build the object that handles training for us: this is instatiated with knowledge of the classifier, normalizer and some identifying features such as the *name* of the experiment and architecture. It's worthwhile to be informative with these so you keep which attacks this model is trained against straight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'tutorial_fgsm'\n",
    "architecture = 'resnet20'\n",
    "training_obj = advtrain.AdversarialTraining(model, normalizer, experiment_name, architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you start training though, you'll need to furnish the trainer with some extra arguments:\n",
    "    - the data loader \n",
    "    - the number of epochs to train for \n",
    "    - a loss function (not one of the `mister_ed` custom loss functions though!)\n",
    "    - which optimizer to use (defaults to Adam with decent hyperparams)\n",
    "    - the attack parameters object \n",
    "    - whether or not to use the gpu (defaults to not using GPU)\n",
    "    - the verbosity level (ranging from ['low', 'medium', 'high', 'snoop'] (defaults to 'medium')\n",
    "    - whether or not to save the generated adversarial examples as images (defaults to false)\n",
    "    \n",
    "To be cute, we'll just train for two epochs so you get the picture. Also note that unless the verbosity is set to `low`, a checkpoint will be saved after every epoch. By default, these checkpoints are named like `<experiment_name>.<architecture_name>.<epoch>.path.tar`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] accuracy: (4.000, 100.000)\n",
      "[1,     1] loss: 1.599944\n",
      "[1,     2] accuracy: (20.000, 100.000)\n",
      "[1,     2] loss: 1.240227\n",
      "[1,     3] accuracy: (16.000, 100.000)\n",
      "[1,     3] loss: 1.249226\n",
      "[1,     4] accuracy: (12.000, 96.000)\n",
      "[1,     4] loss: 1.179514\n",
      "[1,     5] accuracy: (12.000, 96.000)\n",
      "[1,     5] loss: 1.209120\n",
      "[1,     6] accuracy: (28.000, 100.000)\n",
      "[1,     6] loss: 0.802554\n",
      "[1,     7] accuracy: (20.000, 100.000)\n",
      "[1,     7] loss: 0.747037\n",
      "[1,     8] accuracy: (8.000, 96.000)\n",
      "[1,     8] loss: 1.002974\n",
      "[1,     9] accuracy: (28.000, 100.000)\n",
      "[1,     9] loss: 0.521626\n",
      "[1,    10] accuracy: (20.000, 100.000)\n",
      "[1,    10] loss: 0.576680\n",
      "[1,    11] accuracy: (28.000, 96.000)\n",
      "[1,    11] loss: 0.480392\n",
      "[1,    12] accuracy: (24.000, 96.000)\n",
      "[1,    12] loss: 0.451429\n",
      "[1,    13] accuracy: (28.000, 92.000)\n",
      "[1,    13] loss: 0.432828\n",
      "[1,    14] accuracy: (20.000, 92.000)\n",
      "[1,    14] loss: 0.476083\n",
      "[1,    15] accuracy: (12.000, 96.000)\n",
      "[1,    15] loss: 0.530713\n",
      "[1,    16] accuracy: (28.000, 84.000)\n",
      "[1,    16] loss: 0.603256\n",
      "[1,    17] accuracy: (16.000, 88.000)\n",
      "[1,    17] loss: 0.521950\n",
      "[1,    18] accuracy: (12.000, 76.000)\n",
      "[1,    18] loss: 0.650527\n",
      "[1,    19] accuracy: (8.000, 88.000)\n",
      "[1,    19] loss: 0.696946\n",
      "[1,    20] accuracy: (16.000, 72.000)\n",
      "[1,    20] loss: 0.503549\n",
      "[1,    21] accuracy: (16.000, 92.000)\n",
      "[1,    21] loss: 0.549434\n",
      "[1,    22] accuracy: (24.000, 92.000)\n",
      "[1,    22] loss: 0.476655\n",
      "[1,    23] accuracy: (20.000, 88.000)\n",
      "[1,    23] loss: 0.458323\n",
      "[1,    24] accuracy: (20.000, 88.000)\n",
      "[1,    24] loss: 0.571910\n",
      "[1,    25] accuracy: (24.000, 84.000)\n",
      "[1,    25] loss: 0.570257\n",
      "[1,    26] accuracy: (28.000, 92.000)\n",
      "[1,    26] loss: 0.516503\n",
      "[1,    27] accuracy: (12.000, 96.000)\n",
      "[1,    27] loss: 0.504315\n",
      "[1,    28] accuracy: (32.000, 92.000)\n",
      "[1,    28] loss: 0.481210\n",
      "[1,    29] accuracy: (28.000, 84.000)\n",
      "[1,    29] loss: 0.459731\n",
      "[1,    30] accuracy: (36.000, 96.000)\n",
      "[1,    30] loss: 0.404075\n",
      "[1,    31] accuracy: (28.000, 96.000)\n",
      "[1,    31] loss: 0.429149\n",
      "[1,    32] accuracy: (20.000, 92.000)\n",
      "[1,    32] loss: 0.508850\n",
      "[1,    33] accuracy: (28.000, 84.000)\n",
      "[1,    33] loss: 0.461922\n",
      "[1,    34] accuracy: (52.000, 100.000)\n",
      "[1,    34] loss: 0.484581\n",
      "[1,    35] accuracy: (24.000, 84.000)\n",
      "[1,    35] loss: 0.527988\n",
      "[1,    36] accuracy: (24.000, 96.000)\n",
      "[1,    36] loss: 0.463800\n",
      "[1,    37] accuracy: (28.000, 96.000)\n",
      "[1,    37] loss: 0.427546\n",
      "[1,    38] accuracy: (20.000, 88.000)\n",
      "[1,    38] loss: 0.458945\n",
      "[1,    39] accuracy: (40.000, 100.000)\n",
      "[1,    39] loss: 0.431362\n",
      "[1,    40] accuracy: (24.000, 84.000)\n",
      "[1,    40] loss: 0.579885\n",
      "[1,    41] accuracy: (44.000, 88.000)\n",
      "[1,    41] loss: 0.445802\n",
      "[1,    42] accuracy: (40.000, 96.000)\n",
      "[1,    42] loss: 0.461600\n",
      "[1,    43] accuracy: (20.000, 96.000)\n",
      "[1,    43] loss: 0.396329\n",
      "[1,    44] accuracy: (12.000, 100.000)\n",
      "[1,    44] loss: 0.457566\n",
      "[1,    45] accuracy: (20.000, 100.000)\n",
      "[1,    45] loss: 0.370237\n",
      "[1,    46] accuracy: (20.000, 88.000)\n",
      "[1,    46] loss: 0.464364\n",
      "[1,    47] accuracy: (48.000, 96.000)\n",
      "[1,    47] loss: 0.350296\n",
      "[1,    48] accuracy: (16.000, 92.000)\n",
      "[1,    48] loss: 0.451376\n",
      "[1,    49] accuracy: (36.000, 100.000)\n",
      "[1,    49] loss: 0.447600\n",
      "[1,    50] accuracy: (20.000, 96.000)\n",
      "[1,    50] loss: 0.430523\n",
      "[1,    51] accuracy: (40.000, 100.000)\n",
      "[1,    51] loss: 0.500091\n",
      "[1,    52] accuracy: (24.000, 96.000)\n",
      "[1,    52] loss: 0.410917\n",
      "[1,    53] accuracy: (28.000, 100.000)\n",
      "[1,    53] loss: 0.387259\n",
      "[1,    54] accuracy: (24.000, 92.000)\n",
      "[1,    54] loss: 0.454614\n",
      "[1,    55] accuracy: (32.000, 100.000)\n",
      "[1,    55] loss: 0.474091\n",
      "[1,    56] accuracy: (36.000, 96.000)\n",
      "[1,    56] loss: 0.427378\n",
      "[1,    57] accuracy: (44.000, 96.000)\n",
      "[1,    57] loss: 0.343350\n",
      "[1,    58] accuracy: (20.000, 92.000)\n",
      "[1,    58] loss: 0.415418\n",
      "[1,    59] accuracy: (24.000, 100.000)\n",
      "[1,    59] loss: 0.461364\n",
      "[1,    60] accuracy: (8.000, 96.000)\n",
      "[1,    60] loss: 0.527741\n",
      "[1,    61] accuracy: (36.000, 100.000)\n",
      "[1,    61] loss: 0.462534\n",
      "[1,    62] accuracy: (28.000, 100.000)\n",
      "[1,    62] loss: 0.405151\n",
      "[1,    63] accuracy: (20.000, 92.000)\n",
      "[1,    63] loss: 0.491228\n",
      "[1,    64] accuracy: (20.000, 100.000)\n",
      "[1,    64] loss: 0.446069\n",
      "[1,    65] accuracy: (28.000, 96.000)\n",
      "[1,    65] loss: 0.338470\n",
      "[1,    66] accuracy: (12.000, 96.000)\n",
      "[1,    66] loss: 0.365408\n",
      "[1,    67] accuracy: (28.000, 96.000)\n",
      "[1,    67] loss: 0.367594\n",
      "[1,    68] accuracy: (52.000, 96.000)\n",
      "[1,    68] loss: 0.346300\n",
      "[1,    69] accuracy: (24.000, 100.000)\n",
      "[1,    69] loss: 0.439238\n",
      "[1,    70] accuracy: (20.000, 96.000)\n",
      "[1,    70] loss: 0.421035\n",
      "[1,    71] accuracy: (24.000, 100.000)\n",
      "[1,    71] loss: 0.369632\n",
      "[1,    72] accuracy: (36.000, 84.000)\n",
      "[1,    72] loss: 0.384157\n",
      "[1,    73] accuracy: (40.000, 96.000)\n",
      "[1,    73] loss: 0.458349\n",
      "[1,    74] accuracy: (36.000, 100.000)\n",
      "[1,    74] loss: 0.341655\n",
      "[1,    75] accuracy: (52.000, 92.000)\n",
      "[1,    75] loss: 0.392884\n",
      "[1,    76] accuracy: (32.000, 84.000)\n",
      "[1,    76] loss: 0.433003\n",
      "[1,    77] accuracy: (36.000, 100.000)\n",
      "[1,    77] loss: 0.366891\n",
      "[1,    78] accuracy: (28.000, 100.000)\n",
      "[1,    78] loss: 0.406599\n",
      "[1,    79] accuracy: (20.000, 100.000)\n",
      "[1,    79] loss: 0.393715\n",
      "[1,    80] accuracy: (20.000, 84.000)\n",
      "[1,    80] loss: 0.478460\n",
      "[1,    81] accuracy: (40.000, 92.000)\n",
      "[1,    81] loss: 0.392752\n",
      "[1,    82] accuracy: (28.000, 84.000)\n",
      "[1,    82] loss: 0.452311\n",
      "[1,    83] accuracy: (32.000, 96.000)\n",
      "[1,    83] loss: 0.368242\n",
      "[1,    84] accuracy: (40.000, 92.000)\n",
      "[1,    84] loss: 0.355517\n",
      "[1,    85] accuracy: (28.000, 100.000)\n",
      "[1,    85] loss: 0.418346\n",
      "[1,    86] accuracy: (32.000, 92.000)\n",
      "[1,    86] loss: 0.338418\n",
      "[1,    87] accuracy: (24.000, 100.000)\n",
      "[1,    87] loss: 0.423327\n",
      "[1,    88] accuracy: (16.000, 100.000)\n",
      "[1,    88] loss: 0.408304\n",
      "[1,    89] accuracy: (32.000, 100.000)\n",
      "[1,    89] loss: 0.459972\n",
      "[1,    90] accuracy: (28.000, 92.000)\n",
      "[1,    90] loss: 0.437167\n",
      "[1,    91] accuracy: (32.000, 96.000)\n",
      "[1,    91] loss: 0.421379\n",
      "[1,    92] accuracy: (24.000, 96.000)\n",
      "[1,    92] loss: 0.424928\n",
      "[1,    93] accuracy: (8.000, 92.000)\n",
      "[1,    93] loss: 0.536624\n",
      "[1,    94] accuracy: (48.000, 96.000)\n",
      "[1,    94] loss: 0.261002\n",
      "[1,    95] accuracy: (28.000, 100.000)\n",
      "[1,    95] loss: 0.456773\n",
      "[1,    96] accuracy: (36.000, 96.000)\n",
      "[1,    96] loss: 0.366647\n",
      "[1,    97] accuracy: (60.000, 100.000)\n",
      "[1,    97] loss: 0.281728\n",
      "[1,    98] accuracy: (32.000, 96.000)\n",
      "[1,    98] loss: 0.385473\n",
      "[1,    99] accuracy: (28.000, 100.000)\n",
      "[1,    99] loss: 0.361360\n",
      "[1,   100] accuracy: (28.000, 100.000)\n",
      "[1,   100] loss: 0.381884\n",
      "[1,   101] accuracy: (36.000, 92.000)\n",
      "[1,   101] loss: 0.358243\n",
      "[1,   102] accuracy: (32.000, 100.000)\n",
      "[1,   102] loss: 0.371308\n",
      "[1,   103] accuracy: (48.000, 92.000)\n",
      "[1,   103] loss: 0.298357\n",
      "[1,   104] accuracy: (24.000, 96.000)\n",
      "[1,   104] loss: 0.395313\n",
      "[1,   105] accuracy: (28.000, 100.000)\n",
      "[1,   105] loss: 0.329057\n",
      "[1,   106] accuracy: (12.000, 92.000)\n",
      "[1,   106] loss: 0.323995\n",
      "[1,   107] accuracy: (28.000, 100.000)\n",
      "[1,   107] loss: 0.346953\n",
      "[1,   108] accuracy: (20.000, 96.000)\n",
      "[1,   108] loss: 0.446856\n",
      "[1,   109] accuracy: (24.000, 100.000)\n",
      "[1,   109] loss: 0.395778\n",
      "[1,   110] accuracy: (28.000, 100.000)\n",
      "[1,   110] loss: 0.441103\n",
      "[1,   111] accuracy: (28.000, 96.000)\n",
      "[1,   111] loss: 0.392132\n",
      "[1,   112] accuracy: (40.000, 100.000)\n",
      "[1,   112] loss: 0.267841\n",
      "[1,   113] accuracy: (28.000, 96.000)\n",
      "[1,   113] loss: 0.391496\n",
      "[1,   114] accuracy: (36.000, 96.000)\n",
      "[1,   114] loss: 0.386282\n",
      "[1,   115] accuracy: (24.000, 100.000)\n",
      "[1,   115] loss: 0.351591\n",
      "[1,   116] accuracy: (28.000, 96.000)\n",
      "[1,   116] loss: 0.387553\n",
      "[1,   117] accuracy: (52.000, 100.000)\n",
      "[1,   117] loss: 0.334182\n",
      "[1,   118] accuracy: (32.000, 96.000)\n",
      "[1,   118] loss: 0.334517\n",
      "[1,   119] accuracy: (40.000, 88.000)\n",
      "[1,   119] loss: 0.341035\n",
      "[1,   120] accuracy: (48.000, 96.000)\n",
      "[1,   120] loss: 0.377618\n",
      "[1,   121] accuracy: (40.000, 96.000)\n",
      "[1,   121] loss: 0.350036\n",
      "[1,   122] accuracy: (36.000, 100.000)\n",
      "[1,   122] loss: 0.407364\n",
      "[1,   123] accuracy: (44.000, 100.000)\n",
      "[1,   123] loss: 0.351731\n",
      "[1,   124] accuracy: (44.000, 100.000)\n",
      "[1,   124] loss: 0.327749\n",
      "[1,   125] accuracy: (20.000, 96.000)\n",
      "[1,   125] loss: 0.473279\n",
      "[1,   126] accuracy: (52.000, 88.000)\n",
      "[1,   126] loss: 0.352587\n",
      "[1,   127] accuracy: (36.000, 84.000)\n",
      "[1,   127] loss: 0.421504\n",
      "[1,   128] accuracy: (24.000, 96.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   128] loss: 0.420743\n",
      "[1,   129] accuracy: (40.000, 100.000)\n",
      "[1,   129] loss: 0.327217\n",
      "[1,   130] accuracy: (24.000, 100.000)\n",
      "[1,   130] loss: 0.368050\n",
      "[1,   131] accuracy: (36.000, 96.000)\n",
      "[1,   131] loss: 0.394432\n",
      "[1,   132] accuracy: (32.000, 100.000)\n",
      "[1,   132] loss: 0.390075\n",
      "[1,   133] accuracy: (28.000, 96.000)\n",
      "[1,   133] loss: 0.405405\n",
      "[1,   134] accuracy: (40.000, 100.000)\n",
      "[1,   134] loss: 0.387147\n",
      "[1,   135] accuracy: (32.000, 100.000)\n",
      "[1,   135] loss: 0.307586\n",
      "[1,   136] accuracy: (16.000, 96.000)\n",
      "[1,   136] loss: 0.470789\n",
      "[1,   137] accuracy: (20.000, 96.000)\n",
      "[1,   137] loss: 0.404960\n",
      "[1,   138] accuracy: (20.000, 100.000)\n",
      "[1,   138] loss: 0.392479\n",
      "[1,   139] accuracy: (56.000, 96.000)\n",
      "[1,   139] loss: 0.355760\n",
      "[1,   140] accuracy: (24.000, 100.000)\n",
      "[1,   140] loss: 0.429897\n",
      "[1,   141] accuracy: (36.000, 96.000)\n",
      "[1,   141] loss: 0.506881\n",
      "[1,   142] accuracy: (20.000, 100.000)\n",
      "[1,   142] loss: 0.345809\n",
      "[1,   143] accuracy: (40.000, 96.000)\n",
      "[1,   143] loss: 0.324644\n",
      "[1,   144] accuracy: (28.000, 96.000)\n",
      "[1,   144] loss: 0.405337\n",
      "[1,   145] accuracy: (36.000, 96.000)\n",
      "[1,   145] loss: 0.414883\n",
      "[1,   146] accuracy: (40.000, 100.000)\n",
      "[1,   146] loss: 0.385962\n",
      "[1,   147] accuracy: (40.000, 92.000)\n",
      "[1,   147] loss: 0.339696\n",
      "[1,   148] accuracy: (24.000, 92.000)\n",
      "[1,   148] loss: 0.429354\n",
      "[1,   149] accuracy: (36.000, 96.000)\n",
      "[1,   149] loss: 0.374056\n",
      "[1,   150] accuracy: (24.000, 92.000)\n",
      "[1,   150] loss: 0.441803\n",
      "[1,   151] accuracy: (28.000, 92.000)\n",
      "[1,   151] loss: 0.393674\n",
      "[1,   152] accuracy: (40.000, 96.000)\n",
      "[1,   152] loss: 0.339578\n",
      "[1,   153] accuracy: (36.000, 100.000)\n",
      "[1,   153] loss: 0.402884\n",
      "[1,   154] accuracy: (36.000, 100.000)\n",
      "[1,   154] loss: 0.338583\n",
      "[1,   155] accuracy: (36.000, 96.000)\n",
      "[1,   155] loss: 0.340535\n",
      "[1,   156] accuracy: (32.000, 96.000)\n",
      "[1,   156] loss: 0.329674\n",
      "[1,   157] accuracy: (28.000, 96.000)\n",
      "[1,   157] loss: 0.367789\n",
      "[1,   158] accuracy: (32.000, 100.000)\n",
      "[1,   158] loss: 0.415974\n",
      "[1,   159] accuracy: (40.000, 100.000)\n",
      "[1,   159] loss: 0.294524\n",
      "[1,   160] accuracy: (32.000, 92.000)\n",
      "[1,   160] loss: 0.354833\n",
      "[1,   161] accuracy: (28.000, 96.000)\n",
      "[1,   161] loss: 0.406750\n",
      "[1,   162] accuracy: (48.000, 96.000)\n",
      "[1,   162] loss: 0.299905\n",
      "[1,   163] accuracy: (32.000, 96.000)\n",
      "[1,   163] loss: 0.336031\n",
      "[1,   164] accuracy: (40.000, 96.000)\n",
      "[1,   164] loss: 0.368548\n",
      "[1,   165] accuracy: (32.000, 96.000)\n",
      "[1,   165] loss: 0.425068\n",
      "[1,   166] accuracy: (32.000, 96.000)\n",
      "[1,   166] loss: 0.360513\n",
      "[1,   167] accuracy: (36.000, 96.000)\n",
      "[1,   167] loss: 0.344444\n",
      "[1,   168] accuracy: (52.000, 100.000)\n",
      "[1,   168] loss: 0.293391\n",
      "[1,   169] accuracy: (28.000, 96.000)\n",
      "[1,   169] loss: 0.434179\n",
      "[1,   170] accuracy: (28.000, 100.000)\n",
      "[1,   170] loss: 0.376198\n",
      "[1,   171] accuracy: (32.000, 100.000)\n",
      "[1,   171] loss: 0.334998\n",
      "[1,   172] accuracy: (40.000, 100.000)\n",
      "[1,   172] loss: 0.352113\n",
      "[1,   173] accuracy: (24.000, 96.000)\n",
      "[1,   173] loss: 0.485822\n",
      "[1,   174] accuracy: (48.000, 100.000)\n",
      "[1,   174] loss: 0.351792\n",
      "[1,   175] accuracy: (32.000, 96.000)\n",
      "[1,   175] loss: 0.301989\n",
      "[1,   176] accuracy: (36.000, 100.000)\n",
      "[1,   176] loss: 0.464749\n",
      "[1,   177] accuracy: (40.000, 100.000)\n",
      "[1,   177] loss: 0.347799\n",
      "[1,   178] accuracy: (36.000, 92.000)\n",
      "[1,   178] loss: 0.351901\n",
      "[1,   179] accuracy: (32.000, 96.000)\n",
      "[1,   179] loss: 0.424551\n",
      "[1,   180] accuracy: (52.000, 100.000)\n",
      "[1,   180] loss: 0.399030\n",
      "[1,   181] accuracy: (28.000, 100.000)\n",
      "[1,   181] loss: 0.364389\n",
      "[1,   182] accuracy: (44.000, 92.000)\n",
      "[1,   182] loss: 0.468336\n",
      "[1,   183] accuracy: (24.000, 96.000)\n",
      "[1,   183] loss: 0.345249\n",
      "[1,   184] accuracy: (32.000, 100.000)\n",
      "[1,   184] loss: 0.338698\n",
      "[1,   185] accuracy: (44.000, 92.000)\n",
      "[1,   185] loss: 0.367287\n",
      "[1,   186] accuracy: (40.000, 100.000)\n",
      "[1,   186] loss: 0.310435\n",
      "[1,   187] accuracy: (36.000, 100.000)\n",
      "[1,   187] loss: 0.353322\n",
      "[1,   188] accuracy: (36.000, 96.000)\n",
      "[1,   188] loss: 0.392240\n",
      "[1,   189] accuracy: (44.000, 100.000)\n",
      "[1,   189] loss: 0.321086\n",
      "[1,   190] accuracy: (32.000, 92.000)\n",
      "[1,   190] loss: 0.395125\n",
      "[1,   191] accuracy: (36.000, 96.000)\n",
      "[1,   191] loss: 0.442802\n",
      "[1,   192] accuracy: (40.000, 96.000)\n",
      "[1,   192] loss: 0.436651\n",
      "[1,   193] accuracy: (44.000, 96.000)\n",
      "[1,   193] loss: 0.419933\n",
      "[1,   194] accuracy: (20.000, 96.000)\n",
      "[1,   194] loss: 0.364781\n",
      "[1,   195] accuracy: (40.000, 88.000)\n",
      "[1,   195] loss: 0.359372\n",
      "[1,   196] accuracy: (56.000, 100.000)\n",
      "[1,   196] loss: 0.287616\n",
      "[1,   197] accuracy: (44.000, 100.000)\n",
      "[1,   197] loss: 0.326124\n",
      "[1,   198] accuracy: (48.000, 92.000)\n",
      "[1,   198] loss: 0.379126\n",
      "[1,   199] accuracy: (44.000, 100.000)\n",
      "[1,   199] loss: 0.276288\n",
      "[1,   200] accuracy: (36.000, 100.000)\n",
      "[1,   200] loss: 0.439406\n",
      "[1,   201] accuracy: (44.000, 100.000)\n",
      "[1,   201] loss: 0.306742\n",
      "[1,   202] accuracy: (28.000, 96.000)\n",
      "[1,   202] loss: 0.357485\n",
      "[1,   203] accuracy: (52.000, 96.000)\n",
      "[1,   203] loss: 0.323542\n",
      "[1,   204] accuracy: (32.000, 96.000)\n",
      "[1,   204] loss: 0.425052\n",
      "[1,   205] accuracy: (36.000, 100.000)\n",
      "[1,   205] loss: 0.373940\n",
      "[1,   206] accuracy: (28.000, 92.000)\n",
      "[1,   206] loss: 0.359424\n",
      "[1,   207] accuracy: (40.000, 100.000)\n",
      "[1,   207] loss: 0.313981\n",
      "[1,   208] accuracy: (52.000, 92.000)\n",
      "[1,   208] loss: 0.436600\n",
      "[1,   209] accuracy: (40.000, 100.000)\n",
      "[1,   209] loss: 0.374382\n",
      "[1,   210] accuracy: (36.000, 96.000)\n",
      "[1,   210] loss: 0.427304\n",
      "[1,   211] accuracy: (40.000, 100.000)\n",
      "[1,   211] loss: 0.317327\n",
      "[1,   212] accuracy: (20.000, 92.000)\n",
      "[1,   212] loss: 0.383641\n",
      "[1,   213] accuracy: (28.000, 84.000)\n",
      "[1,   213] loss: 0.383115\n",
      "[1,   214] accuracy: (52.000, 84.000)\n",
      "[1,   214] loss: 0.374850\n",
      "[1,   215] accuracy: (28.000, 96.000)\n",
      "[1,   215] loss: 0.419611\n",
      "[1,   216] accuracy: (32.000, 96.000)\n",
      "[1,   216] loss: 0.406894\n",
      "[1,   217] accuracy: (48.000, 92.000)\n",
      "[1,   217] loss: 0.400651\n",
      "[1,   218] accuracy: (56.000, 100.000)\n",
      "[1,   218] loss: 0.373188\n",
      "[1,   219] accuracy: (44.000, 92.000)\n",
      "[1,   219] loss: 0.330639\n",
      "[1,   220] accuracy: (44.000, 92.000)\n",
      "[1,   220] loss: 0.449362\n",
      "[1,   221] accuracy: (40.000, 92.000)\n",
      "[1,   221] loss: 0.350828\n",
      "[1,   222] accuracy: (24.000, 100.000)\n",
      "[1,   222] loss: 0.423406\n",
      "[1,   223] accuracy: (40.000, 100.000)\n",
      "[1,   223] loss: 0.369778\n",
      "[1,   224] accuracy: (44.000, 92.000)\n",
      "[1,   224] loss: 0.312784\n",
      "[1,   225] accuracy: (28.000, 96.000)\n",
      "[1,   225] loss: 0.399019\n",
      "[1,   226] accuracy: (40.000, 100.000)\n",
      "[1,   226] loss: 0.360322\n",
      "[1,   227] accuracy: (40.000, 100.000)\n",
      "[1,   227] loss: 0.381244\n",
      "[1,   228] accuracy: (52.000, 92.000)\n",
      "[1,   228] loss: 0.372349\n",
      "[1,   229] accuracy: (48.000, 96.000)\n",
      "[1,   229] loss: 0.359197\n",
      "[1,   230] accuracy: (32.000, 100.000)\n",
      "[1,   230] loss: 0.393915\n",
      "[1,   231] accuracy: (28.000, 100.000)\n",
      "[1,   231] loss: 0.367508\n",
      "[1,   232] accuracy: (40.000, 84.000)\n",
      "[1,   232] loss: 0.371366\n",
      "[1,   233] accuracy: (32.000, 96.000)\n",
      "[1,   233] loss: 0.321212\n",
      "[1,   234] accuracy: (32.000, 100.000)\n",
      "[1,   234] loss: 0.321262\n",
      "[1,   235] accuracy: (32.000, 96.000)\n",
      "[1,   235] loss: 0.396007\n",
      "[1,   236] accuracy: (28.000, 100.000)\n",
      "[1,   236] loss: 0.386656\n",
      "[1,   237] accuracy: (48.000, 92.000)\n",
      "[1,   237] loss: 0.359848\n",
      "[1,   238] accuracy: (44.000, 92.000)\n",
      "[1,   238] loss: 0.342175\n",
      "[1,   239] accuracy: (48.000, 100.000)\n",
      "[1,   239] loss: 0.389000\n",
      "[1,   240] accuracy: (64.000, 96.000)\n",
      "[1,   240] loss: 0.312694\n",
      "[1,   241] accuracy: (48.000, 96.000)\n",
      "[1,   241] loss: 0.293656\n",
      "[1,   242] accuracy: (28.000, 96.000)\n",
      "[1,   242] loss: 0.305894\n",
      "[1,   243] accuracy: (40.000, 100.000)\n",
      "[1,   243] loss: 0.350401\n",
      "[1,   244] accuracy: (44.000, 100.000)\n",
      "[1,   244] loss: 0.319006\n",
      "[1,   245] accuracy: (32.000, 92.000)\n",
      "[1,   245] loss: 0.359759\n",
      "[1,   246] accuracy: (28.000, 96.000)\n",
      "[1,   246] loss: 0.391485\n",
      "[1,   247] accuracy: (24.000, 100.000)\n",
      "[1,   247] loss: 0.370906\n",
      "[1,   248] accuracy: (40.000, 96.000)\n",
      "[1,   248] loss: 0.401848\n",
      "[1,   249] accuracy: (44.000, 96.000)\n",
      "[1,   249] loss: 0.345487\n",
      "[1,   250] accuracy: (24.000, 92.000)\n",
      "[1,   250] loss: 0.386789\n",
      "[1,   251] accuracy: (36.000, 100.000)\n",
      "[1,   251] loss: 0.296274\n",
      "[1,   252] accuracy: (52.000, 92.000)\n",
      "[1,   252] loss: 0.372860\n",
      "[1,   253] accuracy: (32.000, 96.000)\n",
      "[1,   253] loss: 0.409486\n",
      "[1,   254] accuracy: (52.000, 96.000)\n",
      "[1,   254] loss: 0.316147\n",
      "[1,   255] accuracy: (36.000, 100.000)\n",
      "[1,   255] loss: 0.269565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   256] accuracy: (36.000, 96.000)\n",
      "[1,   256] loss: 0.456183\n",
      "[1,   257] accuracy: (36.000, 92.000)\n",
      "[1,   257] loss: 0.368351\n",
      "[1,   258] accuracy: (44.000, 96.000)\n",
      "[1,   258] loss: 0.277809\n",
      "[1,   259] accuracy: (48.000, 96.000)\n",
      "[1,   259] loss: 0.369027\n",
      "[1,   260] accuracy: (28.000, 96.000)\n",
      "[1,   260] loss: 0.399493\n",
      "[1,   261] accuracy: (40.000, 96.000)\n",
      "[1,   261] loss: 0.402053\n",
      "[1,   262] accuracy: (52.000, 100.000)\n",
      "[1,   262] loss: 0.350295\n",
      "[1,   263] accuracy: (36.000, 92.000)\n",
      "[1,   263] loss: 0.301674\n",
      "[1,   264] accuracy: (20.000, 100.000)\n",
      "[1,   264] loss: 0.412036\n",
      "[1,   265] accuracy: (40.000, 100.000)\n",
      "[1,   265] loss: 0.336813\n",
      "[1,   266] accuracy: (28.000, 100.000)\n",
      "[1,   266] loss: 0.384384\n",
      "[1,   267] accuracy: (40.000, 100.000)\n",
      "[1,   267] loss: 0.338615\n",
      "[1,   268] accuracy: (36.000, 96.000)\n",
      "[1,   268] loss: 0.359784\n",
      "[1,   269] accuracy: (40.000, 88.000)\n",
      "[1,   269] loss: 0.374760\n",
      "[1,   270] accuracy: (32.000, 92.000)\n",
      "[1,   270] loss: 0.366776\n",
      "[1,   271] accuracy: (32.000, 100.000)\n",
      "[1,   271] loss: 0.383303\n",
      "[1,   272] accuracy: (24.000, 92.000)\n",
      "[1,   272] loss: 0.447097\n",
      "[1,   273] accuracy: (4.000, 96.000)\n",
      "[1,   273] loss: 0.522322\n",
      "[1,   274] accuracy: (28.000, 96.000)\n",
      "[1,   274] loss: 0.376494\n",
      "[1,   275] accuracy: (36.000, 96.000)\n",
      "[1,   275] loss: 0.396665\n",
      "[1,   276] accuracy: (44.000, 92.000)\n",
      "[1,   276] loss: 0.424029\n",
      "[1,   277] accuracy: (64.000, 96.000)\n",
      "[1,   277] loss: 0.259400\n",
      "[1,   278] accuracy: (40.000, 96.000)\n",
      "[1,   278] loss: 0.311687\n",
      "[1,   279] accuracy: (28.000, 96.000)\n",
      "[1,   279] loss: 0.366449\n",
      "[1,   280] accuracy: (44.000, 100.000)\n",
      "[1,   280] loss: 0.338006\n",
      "[1,   281] accuracy: (16.000, 100.000)\n",
      "[1,   281] loss: 0.356294\n",
      "[1,   282] accuracy: (60.000, 92.000)\n",
      "[1,   282] loss: 0.318312\n",
      "[1,   283] accuracy: (40.000, 96.000)\n",
      "[1,   283] loss: 0.289800\n",
      "[1,   284] accuracy: (36.000, 96.000)\n",
      "[1,   284] loss: 0.447033\n",
      "[1,   285] accuracy: (48.000, 100.000)\n",
      "[1,   285] loss: 0.409765\n",
      "[1,   286] accuracy: (36.000, 92.000)\n",
      "[1,   286] loss: 0.390215\n",
      "[1,   287] accuracy: (40.000, 88.000)\n",
      "[1,   287] loss: 0.424117\n",
      "[1,   288] accuracy: (36.000, 100.000)\n",
      "[1,   288] loss: 0.422244\n",
      "[1,   289] accuracy: (32.000, 100.000)\n",
      "[1,   289] loss: 0.343999\n",
      "[1,   290] accuracy: (68.000, 100.000)\n",
      "[1,   290] loss: 0.259396\n",
      "[1,   291] accuracy: (32.000, 92.000)\n",
      "[1,   291] loss: 0.464835\n",
      "[1,   292] accuracy: (48.000, 96.000)\n",
      "[1,   292] loss: 0.334825\n",
      "[1,   293] accuracy: (36.000, 100.000)\n",
      "[1,   293] loss: 0.383682\n",
      "[1,   294] accuracy: (36.000, 100.000)\n",
      "[1,   294] loss: 0.281000\n",
      "[1,   295] accuracy: (28.000, 88.000)\n",
      "[1,   295] loss: 0.514932\n",
      "[1,   296] accuracy: (36.000, 100.000)\n",
      "[1,   296] loss: 0.477483\n",
      "[1,   297] accuracy: (52.000, 100.000)\n",
      "[1,   297] loss: 0.294596\n",
      "[1,   298] accuracy: (48.000, 96.000)\n",
      "[1,   298] loss: 0.348461\n",
      "[1,   299] accuracy: (28.000, 92.000)\n",
      "[1,   299] loss: 0.408588\n",
      "[1,   300] accuracy: (48.000, 92.000)\n",
      "[1,   300] loss: 0.430079\n",
      "[1,   301] accuracy: (40.000, 92.000)\n",
      "[1,   301] loss: 0.325630\n",
      "[1,   302] accuracy: (36.000, 96.000)\n",
      "[1,   302] loss: 0.371908\n",
      "[1,   303] accuracy: (36.000, 100.000)\n",
      "[1,   303] loss: 0.410040\n",
      "[1,   304] accuracy: (32.000, 96.000)\n",
      "[1,   304] loss: 0.342365\n",
      "[1,   305] accuracy: (56.000, 96.000)\n",
      "[1,   305] loss: 0.400346\n",
      "[1,   306] accuracy: (32.000, 96.000)\n",
      "[1,   306] loss: 0.398739\n",
      "[1,   307] accuracy: (44.000, 100.000)\n",
      "[1,   307] loss: 0.268980\n",
      "[1,   308] accuracy: (28.000, 100.000)\n",
      "[1,   308] loss: 0.362969\n",
      "[1,   309] accuracy: (40.000, 100.000)\n",
      "[1,   309] loss: 0.469262\n",
      "[1,   310] accuracy: (52.000, 96.000)\n",
      "[1,   310] loss: 0.374381\n",
      "[1,   311] accuracy: (52.000, 96.000)\n",
      "[1,   311] loss: 0.235529\n",
      "[1,   312] accuracy: (32.000, 96.000)\n",
      "[1,   312] loss: 0.365091\n",
      "[1,   313] accuracy: (36.000, 84.000)\n",
      "[1,   313] loss: 0.354463\n",
      "[1,   314] accuracy: (40.000, 92.000)\n",
      "[1,   314] loss: 0.382139\n",
      "[1,   315] accuracy: (36.000, 92.000)\n",
      "[1,   315] loss: 0.273372\n",
      "[1,   316] accuracy: (52.000, 96.000)\n",
      "[1,   316] loss: 0.291097\n",
      "[1,   317] accuracy: (44.000, 96.000)\n",
      "[1,   317] loss: 0.461544\n",
      "[1,   318] accuracy: (40.000, 100.000)\n",
      "[1,   318] loss: 0.334037\n",
      "[1,   319] accuracy: (28.000, 96.000)\n",
      "[1,   319] loss: 0.481380\n",
      "[1,   320] accuracy: (40.000, 92.000)\n",
      "[1,   320] loss: 0.380989\n",
      "[1,   321] accuracy: (36.000, 100.000)\n",
      "[1,   321] loss: 0.322220\n",
      "[1,   322] accuracy: (48.000, 92.000)\n",
      "[1,   322] loss: 0.351032\n",
      "[1,   323] accuracy: (40.000, 100.000)\n",
      "[1,   323] loss: 0.313651\n",
      "[1,   324] accuracy: (32.000, 92.000)\n",
      "[1,   324] loss: 0.367779\n",
      "[1,   325] accuracy: (48.000, 100.000)\n",
      "[1,   325] loss: 0.309869\n",
      "[1,   326] accuracy: (32.000, 100.000)\n",
      "[1,   326] loss: 0.358954\n",
      "[1,   327] accuracy: (52.000, 96.000)\n",
      "[1,   327] loss: 0.341056\n",
      "[1,   328] accuracy: (32.000, 96.000)\n",
      "[1,   328] loss: 0.397511\n",
      "[1,   329] accuracy: (36.000, 100.000)\n",
      "[1,   329] loss: 0.328977\n",
      "[1,   330] accuracy: (52.000, 96.000)\n",
      "[1,   330] loss: 0.320481\n",
      "[1,   331] accuracy: (40.000, 100.000)\n",
      "[1,   331] loss: 0.262455\n",
      "[1,   332] accuracy: (36.000, 96.000)\n",
      "[1,   332] loss: 0.354957\n",
      "[1,   333] accuracy: (40.000, 100.000)\n",
      "[1,   333] loss: 0.336435\n",
      "[1,   334] accuracy: (36.000, 92.000)\n",
      "[1,   334] loss: 0.420133\n",
      "[1,   335] accuracy: (48.000, 100.000)\n",
      "[1,   335] loss: 0.336944\n",
      "[1,   336] accuracy: (28.000, 92.000)\n",
      "[1,   336] loss: 0.362545\n",
      "[1,   337] accuracy: (40.000, 92.000)\n",
      "[1,   337] loss: 0.381876\n",
      "[1,   338] accuracy: (44.000, 96.000)\n",
      "[1,   338] loss: 0.355332\n",
      "[1,   339] accuracy: (40.000, 100.000)\n",
      "[1,   339] loss: 0.351030\n",
      "[1,   340] accuracy: (44.000, 96.000)\n",
      "[1,   340] loss: 0.386652\n",
      "[1,   341] accuracy: (40.000, 92.000)\n",
      "[1,   341] loss: 0.369747\n",
      "[1,   342] accuracy: (56.000, 100.000)\n",
      "[1,   342] loss: 0.350013\n",
      "[1,   343] accuracy: (44.000, 92.000)\n",
      "[1,   343] loss: 0.397487\n",
      "[1,   344] accuracy: (48.000, 96.000)\n",
      "[1,   344] loss: 0.375659\n",
      "[1,   345] accuracy: (40.000, 92.000)\n",
      "[1,   345] loss: 0.368089\n",
      "[1,   346] accuracy: (28.000, 88.000)\n",
      "[1,   346] loss: 0.377085\n",
      "[1,   347] accuracy: (36.000, 92.000)\n",
      "[1,   347] loss: 0.366462\n",
      "[1,   348] accuracy: (28.000, 92.000)\n",
      "[1,   348] loss: 0.388621\n",
      "[1,   349] accuracy: (36.000, 96.000)\n",
      "[1,   349] loss: 0.395128\n",
      "[1,   350] accuracy: (48.000, 88.000)\n",
      "[1,   350] loss: 0.326517\n",
      "[1,   351] accuracy: (44.000, 100.000)\n",
      "[1,   351] loss: 0.418621\n",
      "[1,   352] accuracy: (32.000, 88.000)\n",
      "[1,   352] loss: 0.405781\n",
      "[1,   353] accuracy: (36.000, 92.000)\n",
      "[1,   353] loss: 0.305027\n",
      "[1,   354] accuracy: (52.000, 92.000)\n",
      "[1,   354] loss: 0.381969\n",
      "[1,   355] accuracy: (48.000, 96.000)\n",
      "[1,   355] loss: 0.399634\n",
      "[1,   356] accuracy: (36.000, 100.000)\n",
      "[1,   356] loss: 0.320240\n",
      "[1,   357] accuracy: (56.000, 96.000)\n",
      "[1,   357] loss: 0.324514\n",
      "[1,   358] accuracy: (52.000, 100.000)\n",
      "[1,   358] loss: 0.320233\n",
      "[1,   359] accuracy: (44.000, 100.000)\n",
      "[1,   359] loss: 0.287574\n",
      "[1,   360] accuracy: (24.000, 100.000)\n",
      "[1,   360] loss: 0.366488\n",
      "[1,   361] accuracy: (44.000, 92.000)\n",
      "[1,   361] loss: 0.392619\n",
      "[1,   362] accuracy: (40.000, 100.000)\n",
      "[1,   362] loss: 0.344881\n",
      "[1,   363] accuracy: (48.000, 92.000)\n",
      "[1,   363] loss: 0.331349\n",
      "[1,   364] accuracy: (44.000, 92.000)\n",
      "[1,   364] loss: 0.353523\n",
      "[1,   365] accuracy: (40.000, 84.000)\n",
      "[1,   365] loss: 0.465904\n",
      "[1,   366] accuracy: (44.000, 100.000)\n",
      "[1,   366] loss: 0.381168\n",
      "[1,   367] accuracy: (36.000, 100.000)\n",
      "[1,   367] loss: 0.303660\n",
      "[1,   368] accuracy: (40.000, 92.000)\n",
      "[1,   368] loss: 0.393899\n",
      "[1,   369] accuracy: (68.000, 96.000)\n",
      "[1,   369] loss: 0.284607\n",
      "[1,   370] accuracy: (32.000, 96.000)\n",
      "[1,   370] loss: 0.448366\n",
      "[1,   371] accuracy: (36.000, 96.000)\n",
      "[1,   371] loss: 0.509881\n",
      "[1,   372] accuracy: (32.000, 100.000)\n",
      "[1,   372] loss: 0.371712\n",
      "[1,   373] accuracy: (40.000, 96.000)\n",
      "[1,   373] loss: 0.351087\n",
      "[1,   374] accuracy: (56.000, 92.000)\n",
      "[1,   374] loss: 0.342482\n",
      "[1,   375] accuracy: (44.000, 92.000)\n",
      "[1,   375] loss: 0.374461\n",
      "[1,   376] accuracy: (60.000, 92.000)\n",
      "[1,   376] loss: 0.291119\n",
      "[1,   377] accuracy: (44.000, 96.000)\n",
      "[1,   377] loss: 0.350353\n",
      "[1,   378] accuracy: (32.000, 96.000)\n",
      "[1,   378] loss: 0.390960\n",
      "[1,   379] accuracy: (40.000, 92.000)\n",
      "[1,   379] loss: 0.386763\n",
      "[1,   380] accuracy: (48.000, 96.000)\n",
      "[1,   380] loss: 0.311934\n",
      "[1,   381] accuracy: (28.000, 96.000)\n",
      "[1,   381] loss: 0.330850\n",
      "[1,   382] accuracy: (44.000, 96.000)\n",
      "[1,   382] loss: 0.390480\n",
      "[1,   383] accuracy: (20.000, 100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   383] loss: 0.358840\n",
      "[1,   384] accuracy: (40.000, 96.000)\n",
      "[1,   384] loss: 0.342919\n",
      "[1,   385] accuracy: (40.000, 92.000)\n",
      "[1,   385] loss: 0.336729\n",
      "[1,   386] accuracy: (40.000, 100.000)\n",
      "[1,   386] loss: 0.353301\n",
      "[1,   387] accuracy: (44.000, 96.000)\n",
      "[1,   387] loss: 0.325844\n",
      "[1,   388] accuracy: (12.000, 96.000)\n",
      "[1,   388] loss: 0.387046\n",
      "[1,   389] accuracy: (60.000, 100.000)\n",
      "[1,   389] loss: 0.340109\n",
      "[1,   390] accuracy: (44.000, 96.000)\n",
      "[1,   390] loss: 0.325224\n",
      "[1,   391] accuracy: (25.000, 93.750)\n",
      "[1,   391] loss: 0.414688\n",
      "COMPLETED EPOCH 0001... checkpointing here\n",
      "[2,     1] accuracy: (44.000, 96.000)\n",
      "[2,     1] loss: 0.357591\n",
      "[2,     2] accuracy: (56.000, 96.000)\n",
      "[2,     2] loss: 0.383988\n",
      "[2,     3] accuracy: (48.000, 100.000)\n",
      "[2,     3] loss: 0.240366\n",
      "[2,     4] accuracy: (24.000, 88.000)\n",
      "[2,     4] loss: 0.376790\n",
      "[2,     5] accuracy: (28.000, 100.000)\n",
      "[2,     5] loss: 0.379570\n",
      "[2,     6] accuracy: (48.000, 96.000)\n",
      "[2,     6] loss: 0.380804\n",
      "[2,     7] accuracy: (36.000, 92.000)\n",
      "[2,     7] loss: 0.353899\n",
      "[2,     8] accuracy: (40.000, 88.000)\n",
      "[2,     8] loss: 0.358803\n",
      "[2,     9] accuracy: (44.000, 100.000)\n",
      "[2,     9] loss: 0.330334\n",
      "[2,    10] accuracy: (28.000, 96.000)\n",
      "[2,    10] loss: 0.373946\n",
      "[2,    11] accuracy: (52.000, 96.000)\n",
      "[2,    11] loss: 0.344225\n",
      "[2,    12] accuracy: (32.000, 96.000)\n",
      "[2,    12] loss: 0.399732\n",
      "[2,    13] accuracy: (44.000, 100.000)\n",
      "[2,    13] loss: 0.281366\n",
      "[2,    14] accuracy: (44.000, 100.000)\n",
      "[2,    14] loss: 0.295228\n",
      "[2,    15] accuracy: (52.000, 100.000)\n",
      "[2,    15] loss: 0.297749\n",
      "[2,    16] accuracy: (28.000, 100.000)\n",
      "[2,    16] loss: 0.343814\n",
      "[2,    17] accuracy: (40.000, 96.000)\n",
      "[2,    17] loss: 0.398174\n",
      "[2,    18] accuracy: (56.000, 96.000)\n",
      "[2,    18] loss: 0.357548\n",
      "[2,    19] accuracy: (64.000, 96.000)\n",
      "[2,    19] loss: 0.283984\n",
      "[2,    20] accuracy: (40.000, 100.000)\n",
      "[2,    20] loss: 0.352115\n",
      "[2,    21] accuracy: (44.000, 92.000)\n",
      "[2,    21] loss: 0.385894\n",
      "[2,    22] accuracy: (40.000, 100.000)\n",
      "[2,    22] loss: 0.275996\n",
      "[2,    23] accuracy: (40.000, 100.000)\n",
      "[2,    23] loss: 0.359286\n",
      "[2,    24] accuracy: (44.000, 96.000)\n",
      "[2,    24] loss: 0.354485\n",
      "[2,    25] accuracy: (44.000, 92.000)\n",
      "[2,    25] loss: 0.367820\n",
      "[2,    26] accuracy: (52.000, 96.000)\n",
      "[2,    26] loss: 0.316201\n",
      "[2,    27] accuracy: (48.000, 88.000)\n",
      "[2,    27] loss: 0.318602\n",
      "[2,    28] accuracy: (52.000, 96.000)\n",
      "[2,    28] loss: 0.291215\n",
      "[2,    29] accuracy: (32.000, 96.000)\n",
      "[2,    29] loss: 0.363082\n",
      "[2,    30] accuracy: (32.000, 100.000)\n",
      "[2,    30] loss: 0.399177\n",
      "[2,    31] accuracy: (60.000, 100.000)\n",
      "[2,    31] loss: 0.240138\n",
      "[2,    32] accuracy: (36.000, 92.000)\n",
      "[2,    32] loss: 0.389577\n",
      "[2,    33] accuracy: (48.000, 92.000)\n",
      "[2,    33] loss: 0.346348\n",
      "[2,    34] accuracy: (40.000, 96.000)\n",
      "[2,    34] loss: 0.325210\n",
      "[2,    35] accuracy: (44.000, 92.000)\n",
      "[2,    35] loss: 0.288452\n",
      "[2,    36] accuracy: (44.000, 96.000)\n",
      "[2,    36] loss: 0.313171\n",
      "[2,    37] accuracy: (60.000, 100.000)\n",
      "[2,    37] loss: 0.260625\n",
      "[2,    38] accuracy: (28.000, 96.000)\n",
      "[2,    38] loss: 0.301181\n",
      "[2,    39] accuracy: (52.000, 96.000)\n",
      "[2,    39] loss: 0.406729\n",
      "[2,    40] accuracy: (44.000, 100.000)\n",
      "[2,    40] loss: 0.319316\n",
      "[2,    41] accuracy: (28.000, 92.000)\n",
      "[2,    41] loss: 0.380642\n",
      "[2,    42] accuracy: (28.000, 96.000)\n",
      "[2,    42] loss: 0.340255\n",
      "[2,    43] accuracy: (28.000, 100.000)\n",
      "[2,    43] loss: 0.339607\n",
      "[2,    44] accuracy: (40.000, 96.000)\n",
      "[2,    44] loss: 0.357329\n",
      "[2,    45] accuracy: (20.000, 96.000)\n",
      "[2,    45] loss: 0.371161\n",
      "[2,    46] accuracy: (32.000, 100.000)\n",
      "[2,    46] loss: 0.347828\n",
      "[2,    47] accuracy: (20.000, 92.000)\n",
      "[2,    47] loss: 0.408462\n",
      "[2,    48] accuracy: (56.000, 92.000)\n",
      "[2,    48] loss: 0.300990\n",
      "[2,    49] accuracy: (56.000, 96.000)\n",
      "[2,    49] loss: 0.372383\n",
      "[2,    50] accuracy: (28.000, 96.000)\n",
      "[2,    50] loss: 0.380126\n",
      "[2,    51] accuracy: (40.000, 100.000)\n",
      "[2,    51] loss: 0.337586\n",
      "[2,    52] accuracy: (44.000, 96.000)\n",
      "[2,    52] loss: 0.419188\n",
      "[2,    53] accuracy: (44.000, 100.000)\n",
      "[2,    53] loss: 0.400847\n",
      "[2,    54] accuracy: (40.000, 100.000)\n",
      "[2,    54] loss: 0.294778\n",
      "[2,    55] accuracy: (36.000, 92.000)\n",
      "[2,    55] loss: 0.313705\n",
      "[2,    56] accuracy: (48.000, 92.000)\n",
      "[2,    56] loss: 0.394506\n",
      "[2,    57] accuracy: (44.000, 96.000)\n",
      "[2,    57] loss: 0.286303\n",
      "[2,    58] accuracy: (60.000, 100.000)\n",
      "[2,    58] loss: 0.226857\n",
      "[2,    59] accuracy: (20.000, 96.000)\n",
      "[2,    59] loss: 0.404429\n",
      "[2,    60] accuracy: (44.000, 96.000)\n",
      "[2,    60] loss: 0.337395\n",
      "[2,    61] accuracy: (28.000, 100.000)\n",
      "[2,    61] loss: 0.339247\n",
      "[2,    62] accuracy: (64.000, 92.000)\n",
      "[2,    62] loss: 0.346568\n",
      "[2,    63] accuracy: (40.000, 100.000)\n",
      "[2,    63] loss: 0.265810\n",
      "[2,    64] accuracy: (24.000, 96.000)\n",
      "[2,    64] loss: 0.393042\n",
      "[2,    65] accuracy: (40.000, 92.000)\n",
      "[2,    65] loss: 0.417185\n",
      "[2,    66] accuracy: (44.000, 96.000)\n",
      "[2,    66] loss: 0.458596\n",
      "[2,    67] accuracy: (52.000, 96.000)\n",
      "[2,    67] loss: 0.257183\n",
      "[2,    68] accuracy: (52.000, 88.000)\n",
      "[2,    68] loss: 0.341291\n",
      "[2,    69] accuracy: (44.000, 96.000)\n",
      "[2,    69] loss: 0.290444\n",
      "[2,    70] accuracy: (68.000, 100.000)\n",
      "[2,    70] loss: 0.299701\n",
      "[2,    71] accuracy: (28.000, 100.000)\n",
      "[2,    71] loss: 0.410514\n",
      "[2,    72] accuracy: (32.000, 92.000)\n",
      "[2,    72] loss: 0.394629\n",
      "[2,    73] accuracy: (44.000, 96.000)\n",
      "[2,    73] loss: 0.375591\n",
      "[2,    74] accuracy: (32.000, 96.000)\n",
      "[2,    74] loss: 0.401164\n",
      "[2,    75] accuracy: (28.000, 100.000)\n",
      "[2,    75] loss: 0.369939\n",
      "[2,    76] accuracy: (48.000, 96.000)\n",
      "[2,    76] loss: 0.299801\n",
      "[2,    77] accuracy: (36.000, 96.000)\n",
      "[2,    77] loss: 0.358441\n",
      "[2,    78] accuracy: (40.000, 96.000)\n",
      "[2,    78] loss: 0.394216\n",
      "[2,    79] accuracy: (60.000, 84.000)\n",
      "[2,    79] loss: 0.336319\n",
      "[2,    80] accuracy: (28.000, 92.000)\n",
      "[2,    80] loss: 0.481828\n",
      "[2,    81] accuracy: (36.000, 96.000)\n",
      "[2,    81] loss: 0.284189\n",
      "[2,    82] accuracy: (44.000, 96.000)\n",
      "[2,    82] loss: 0.391321\n",
      "[2,    83] accuracy: (24.000, 96.000)\n",
      "[2,    83] loss: 0.390069\n",
      "[2,    84] accuracy: (48.000, 100.000)\n",
      "[2,    84] loss: 0.339181\n",
      "[2,    85] accuracy: (48.000, 92.000)\n",
      "[2,    85] loss: 0.333168\n",
      "[2,    86] accuracy: (36.000, 100.000)\n",
      "[2,    86] loss: 0.315563\n",
      "[2,    87] accuracy: (32.000, 88.000)\n",
      "[2,    87] loss: 0.424075\n",
      "[2,    88] accuracy: (36.000, 88.000)\n",
      "[2,    88] loss: 0.354440\n",
      "[2,    89] accuracy: (56.000, 96.000)\n",
      "[2,    89] loss: 0.346648\n",
      "[2,    90] accuracy: (36.000, 100.000)\n",
      "[2,    90] loss: 0.332241\n",
      "[2,    91] accuracy: (40.000, 96.000)\n",
      "[2,    91] loss: 0.327455\n",
      "[2,    92] accuracy: (32.000, 100.000)\n",
      "[2,    92] loss: 0.314988\n",
      "[2,    93] accuracy: (48.000, 100.000)\n",
      "[2,    93] loss: 0.302677\n",
      "[2,    94] accuracy: (36.000, 96.000)\n",
      "[2,    94] loss: 0.379242\n",
      "[2,    95] accuracy: (44.000, 100.000)\n",
      "[2,    95] loss: 0.294723\n",
      "[2,    96] accuracy: (48.000, 96.000)\n",
      "[2,    96] loss: 0.325883\n",
      "[2,    97] accuracy: (40.000, 92.000)\n",
      "[2,    97] loss: 0.401858\n",
      "[2,    98] accuracy: (48.000, 100.000)\n",
      "[2,    98] loss: 0.287578\n",
      "[2,    99] accuracy: (44.000, 88.000)\n",
      "[2,    99] loss: 0.357997\n",
      "[2,   100] accuracy: (52.000, 96.000)\n",
      "[2,   100] loss: 0.323372\n",
      "[2,   101] accuracy: (44.000, 92.000)\n",
      "[2,   101] loss: 0.411157\n",
      "[2,   102] accuracy: (36.000, 92.000)\n",
      "[2,   102] loss: 0.321244\n",
      "[2,   103] accuracy: (48.000, 96.000)\n",
      "[2,   103] loss: 0.296278\n",
      "[2,   104] accuracy: (48.000, 100.000)\n",
      "[2,   104] loss: 0.294113\n",
      "[2,   105] accuracy: (52.000, 96.000)\n",
      "[2,   105] loss: 0.268308\n",
      "[2,   106] accuracy: (40.000, 100.000)\n",
      "[2,   106] loss: 0.266774\n",
      "[2,   107] accuracy: (48.000, 96.000)\n",
      "[2,   107] loss: 0.387001\n",
      "[2,   108] accuracy: (32.000, 96.000)\n",
      "[2,   108] loss: 0.310777\n",
      "[2,   109] accuracy: (48.000, 96.000)\n",
      "[2,   109] loss: 0.326476\n",
      "[2,   110] accuracy: (52.000, 92.000)\n",
      "[2,   110] loss: 0.336240\n",
      "[2,   111] accuracy: (52.000, 100.000)\n",
      "[2,   111] loss: 0.271759\n",
      "[2,   112] accuracy: (44.000, 100.000)\n",
      "[2,   112] loss: 0.345815\n",
      "[2,   113] accuracy: (40.000, 96.000)\n",
      "[2,   113] loss: 0.356262\n",
      "[2,   114] accuracy: (36.000, 92.000)\n",
      "[2,   114] loss: 0.305410\n",
      "[2,   115] accuracy: (48.000, 96.000)\n",
      "[2,   115] loss: 0.266509\n",
      "[2,   116] accuracy: (28.000, 88.000)\n",
      "[2,   116] loss: 0.471143\n",
      "[2,   117] accuracy: (64.000, 100.000)\n",
      "[2,   117] loss: 0.271690\n",
      "[2,   118] accuracy: (44.000, 96.000)\n",
      "[2,   118] loss: 0.281325\n",
      "[2,   119] accuracy: (56.000, 96.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   119] loss: 0.301239\n",
      "[2,   120] accuracy: (60.000, 100.000)\n",
      "[2,   120] loss: 0.307872\n",
      "[2,   121] accuracy: (48.000, 96.000)\n",
      "[2,   121] loss: 0.299466\n",
      "[2,   122] accuracy: (48.000, 92.000)\n",
      "[2,   122] loss: 0.409709\n",
      "[2,   123] accuracy: (28.000, 100.000)\n",
      "[2,   123] loss: 0.341992\n",
      "[2,   124] accuracy: (72.000, 100.000)\n",
      "[2,   124] loss: 0.287945\n",
      "[2,   125] accuracy: (32.000, 96.000)\n",
      "[2,   125] loss: 0.356749\n",
      "[2,   126] accuracy: (56.000, 96.000)\n",
      "[2,   126] loss: 0.269052\n",
      "[2,   127] accuracy: (32.000, 100.000)\n",
      "[2,   127] loss: 0.449142\n",
      "[2,   128] accuracy: (56.000, 96.000)\n",
      "[2,   128] loss: 0.300629\n",
      "[2,   129] accuracy: (44.000, 96.000)\n",
      "[2,   129] loss: 0.325262\n",
      "[2,   130] accuracy: (40.000, 100.000)\n",
      "[2,   130] loss: 0.351304\n",
      "[2,   131] accuracy: (56.000, 96.000)\n",
      "[2,   131] loss: 0.299945\n",
      "[2,   132] accuracy: (52.000, 88.000)\n",
      "[2,   132] loss: 0.416187\n",
      "[2,   133] accuracy: (24.000, 96.000)\n",
      "[2,   133] loss: 0.379433\n",
      "[2,   134] accuracy: (40.000, 100.000)\n",
      "[2,   134] loss: 0.281973\n",
      "[2,   135] accuracy: (64.000, 92.000)\n",
      "[2,   135] loss: 0.251503\n",
      "[2,   136] accuracy: (44.000, 92.000)\n",
      "[2,   136] loss: 0.369963\n",
      "[2,   137] accuracy: (36.000, 100.000)\n",
      "[2,   137] loss: 0.419280\n",
      "[2,   138] accuracy: (44.000, 96.000)\n",
      "[2,   138] loss: 0.373004\n",
      "[2,   139] accuracy: (52.000, 96.000)\n",
      "[2,   139] loss: 0.278588\n",
      "[2,   140] accuracy: (72.000, 96.000)\n",
      "[2,   140] loss: 0.231922\n",
      "[2,   141] accuracy: (40.000, 96.000)\n",
      "[2,   141] loss: 0.379991\n",
      "[2,   142] accuracy: (64.000, 88.000)\n",
      "[2,   142] loss: 0.357012\n",
      "[2,   143] accuracy: (64.000, 92.000)\n",
      "[2,   143] loss: 0.270658\n",
      "[2,   144] accuracy: (48.000, 96.000)\n",
      "[2,   144] loss: 0.311499\n",
      "[2,   145] accuracy: (36.000, 88.000)\n",
      "[2,   145] loss: 0.322070\n",
      "[2,   146] accuracy: (24.000, 88.000)\n",
      "[2,   146] loss: 0.438478\n",
      "[2,   147] accuracy: (56.000, 96.000)\n",
      "[2,   147] loss: 0.344078\n",
      "[2,   148] accuracy: (56.000, 88.000)\n",
      "[2,   148] loss: 0.264939\n",
      "[2,   149] accuracy: (36.000, 92.000)\n",
      "[2,   149] loss: 0.346644\n",
      "[2,   150] accuracy: (48.000, 100.000)\n",
      "[2,   150] loss: 0.284923\n",
      "[2,   151] accuracy: (36.000, 96.000)\n",
      "[2,   151] loss: 0.368782\n",
      "[2,   152] accuracy: (40.000, 96.000)\n",
      "[2,   152] loss: 0.288589\n",
      "[2,   153] accuracy: (48.000, 96.000)\n",
      "[2,   153] loss: 0.336414\n",
      "[2,   154] accuracy: (52.000, 92.000)\n",
      "[2,   154] loss: 0.299231\n",
      "[2,   155] accuracy: (36.000, 96.000)\n",
      "[2,   155] loss: 0.370621\n",
      "[2,   156] accuracy: (36.000, 92.000)\n",
      "[2,   156] loss: 0.390503\n",
      "[2,   157] accuracy: (16.000, 96.000)\n",
      "[2,   157] loss: 0.443221\n",
      "[2,   158] accuracy: (52.000, 100.000)\n",
      "[2,   158] loss: 0.246957\n",
      "[2,   159] accuracy: (36.000, 92.000)\n",
      "[2,   159] loss: 0.293105\n",
      "[2,   160] accuracy: (52.000, 100.000)\n",
      "[2,   160] loss: 0.299533\n",
      "[2,   161] accuracy: (44.000, 100.000)\n",
      "[2,   161] loss: 0.242147\n",
      "[2,   162] accuracy: (36.000, 96.000)\n",
      "[2,   162] loss: 0.387032\n",
      "[2,   163] accuracy: (52.000, 100.000)\n",
      "[2,   163] loss: 0.412444\n",
      "[2,   164] accuracy: (40.000, 96.000)\n",
      "[2,   164] loss: 0.391669\n",
      "[2,   165] accuracy: (40.000, 96.000)\n",
      "[2,   165] loss: 0.354330\n",
      "[2,   166] accuracy: (48.000, 92.000)\n",
      "[2,   166] loss: 0.361608\n",
      "[2,   167] accuracy: (28.000, 100.000)\n",
      "[2,   167] loss: 0.362283\n",
      "[2,   168] accuracy: (40.000, 100.000)\n",
      "[2,   168] loss: 0.274014\n",
      "[2,   169] accuracy: (56.000, 100.000)\n",
      "[2,   169] loss: 0.258536\n",
      "[2,   170] accuracy: (64.000, 100.000)\n",
      "[2,   170] loss: 0.245487\n",
      "[2,   171] accuracy: (48.000, 100.000)\n",
      "[2,   171] loss: 0.352125\n",
      "[2,   172] accuracy: (48.000, 100.000)\n",
      "[2,   172] loss: 0.247277\n",
      "[2,   173] accuracy: (36.000, 100.000)\n",
      "[2,   173] loss: 0.327399\n",
      "[2,   174] accuracy: (44.000, 100.000)\n",
      "[2,   174] loss: 0.422491\n",
      "[2,   175] accuracy: (40.000, 92.000)\n",
      "[2,   175] loss: 0.357980\n",
      "[2,   176] accuracy: (44.000, 84.000)\n",
      "[2,   176] loss: 0.488460\n",
      "[2,   177] accuracy: (40.000, 92.000)\n",
      "[2,   177] loss: 0.367763\n",
      "[2,   178] accuracy: (52.000, 96.000)\n",
      "[2,   178] loss: 0.295359\n",
      "[2,   179] accuracy: (56.000, 100.000)\n",
      "[2,   179] loss: 0.318011\n",
      "[2,   180] accuracy: (44.000, 100.000)\n",
      "[2,   180] loss: 0.360556\n",
      "[2,   181] accuracy: (48.000, 92.000)\n",
      "[2,   181] loss: 0.297275\n",
      "[2,   182] accuracy: (40.000, 96.000)\n",
      "[2,   182] loss: 0.323608\n",
      "[2,   183] accuracy: (28.000, 100.000)\n",
      "[2,   183] loss: 0.374204\n",
      "[2,   184] accuracy: (40.000, 96.000)\n",
      "[2,   184] loss: 0.395488\n",
      "[2,   185] accuracy: (32.000, 96.000)\n",
      "[2,   185] loss: 0.341972\n",
      "[2,   186] accuracy: (52.000, 96.000)\n",
      "[2,   186] loss: 0.313710\n",
      "[2,   187] accuracy: (48.000, 96.000)\n",
      "[2,   187] loss: 0.297102\n",
      "[2,   188] accuracy: (32.000, 96.000)\n",
      "[2,   188] loss: 0.362016\n",
      "[2,   189] accuracy: (48.000, 92.000)\n",
      "[2,   189] loss: 0.374050\n",
      "[2,   190] accuracy: (44.000, 100.000)\n",
      "[2,   190] loss: 0.329568\n",
      "[2,   191] accuracy: (40.000, 88.000)\n",
      "[2,   191] loss: 0.314639\n",
      "[2,   192] accuracy: (60.000, 96.000)\n",
      "[2,   192] loss: 0.300533\n",
      "[2,   193] accuracy: (28.000, 92.000)\n",
      "[2,   193] loss: 0.397557\n",
      "[2,   194] accuracy: (52.000, 96.000)\n",
      "[2,   194] loss: 0.275240\n",
      "[2,   195] accuracy: (32.000, 88.000)\n",
      "[2,   195] loss: 0.302138\n",
      "[2,   196] accuracy: (36.000, 100.000)\n",
      "[2,   196] loss: 0.345629\n",
      "[2,   197] accuracy: (44.000, 96.000)\n",
      "[2,   197] loss: 0.262039\n",
      "[2,   198] accuracy: (48.000, 96.000)\n",
      "[2,   198] loss: 0.342001\n",
      "[2,   199] accuracy: (40.000, 96.000)\n",
      "[2,   199] loss: 0.398752\n",
      "[2,   200] accuracy: (28.000, 84.000)\n",
      "[2,   200] loss: 0.400313\n",
      "[2,   201] accuracy: (36.000, 96.000)\n",
      "[2,   201] loss: 0.389770\n",
      "[2,   202] accuracy: (28.000, 96.000)\n",
      "[2,   202] loss: 0.402649\n",
      "[2,   203] accuracy: (52.000, 100.000)\n",
      "[2,   203] loss: 0.304961\n",
      "[2,   204] accuracy: (40.000, 92.000)\n",
      "[2,   204] loss: 0.307866\n",
      "[2,   205] accuracy: (40.000, 92.000)\n",
      "[2,   205] loss: 0.396037\n",
      "[2,   206] accuracy: (32.000, 100.000)\n",
      "[2,   206] loss: 0.318784\n",
      "[2,   207] accuracy: (52.000, 96.000)\n",
      "[2,   207] loss: 0.318544\n",
      "[2,   208] accuracy: (40.000, 96.000)\n",
      "[2,   208] loss: 0.403080\n",
      "[2,   209] accuracy: (44.000, 92.000)\n",
      "[2,   209] loss: 0.337905\n",
      "[2,   210] accuracy: (52.000, 92.000)\n",
      "[2,   210] loss: 0.320643\n",
      "[2,   211] accuracy: (44.000, 100.000)\n",
      "[2,   211] loss: 0.296621\n",
      "[2,   212] accuracy: (52.000, 92.000)\n",
      "[2,   212] loss: 0.337411\n",
      "[2,   213] accuracy: (48.000, 100.000)\n",
      "[2,   213] loss: 0.258429\n",
      "[2,   214] accuracy: (48.000, 96.000)\n",
      "[2,   214] loss: 0.307333\n",
      "[2,   215] accuracy: (56.000, 100.000)\n",
      "[2,   215] loss: 0.280494\n",
      "[2,   216] accuracy: (40.000, 100.000)\n",
      "[2,   216] loss: 0.320048\n",
      "[2,   217] accuracy: (48.000, 92.000)\n",
      "[2,   217] loss: 0.331603\n",
      "[2,   218] accuracy: (60.000, 100.000)\n",
      "[2,   218] loss: 0.320503\n",
      "[2,   219] accuracy: (56.000, 100.000)\n",
      "[2,   219] loss: 0.356840\n",
      "[2,   220] accuracy: (44.000, 96.000)\n",
      "[2,   220] loss: 0.346592\n",
      "[2,   221] accuracy: (44.000, 96.000)\n",
      "[2,   221] loss: 0.322734\n",
      "[2,   222] accuracy: (48.000, 96.000)\n",
      "[2,   222] loss: 0.315728\n",
      "[2,   223] accuracy: (40.000, 96.000)\n",
      "[2,   223] loss: 0.331856\n",
      "[2,   224] accuracy: (40.000, 100.000)\n",
      "[2,   224] loss: 0.349062\n",
      "[2,   225] accuracy: (52.000, 100.000)\n",
      "[2,   225] loss: 0.309713\n",
      "[2,   226] accuracy: (56.000, 92.000)\n",
      "[2,   226] loss: 0.412661\n",
      "[2,   227] accuracy: (32.000, 96.000)\n",
      "[2,   227] loss: 0.362634\n",
      "[2,   228] accuracy: (56.000, 92.000)\n",
      "[2,   228] loss: 0.250564\n",
      "[2,   229] accuracy: (36.000, 96.000)\n",
      "[2,   229] loss: 0.337329\n",
      "[2,   230] accuracy: (52.000, 92.000)\n",
      "[2,   230] loss: 0.289551\n",
      "[2,   231] accuracy: (44.000, 96.000)\n",
      "[2,   231] loss: 0.488294\n",
      "[2,   232] accuracy: (52.000, 96.000)\n",
      "[2,   232] loss: 0.373533\n",
      "[2,   233] accuracy: (16.000, 96.000)\n",
      "[2,   233] loss: 0.362852\n",
      "[2,   234] accuracy: (56.000, 100.000)\n",
      "[2,   234] loss: 0.240645\n",
      "[2,   235] accuracy: (36.000, 92.000)\n",
      "[2,   235] loss: 0.367641\n",
      "[2,   236] accuracy: (36.000, 96.000)\n",
      "[2,   236] loss: 0.326720\n",
      "[2,   237] accuracy: (44.000, 96.000)\n",
      "[2,   237] loss: 0.353823\n",
      "[2,   238] accuracy: (52.000, 100.000)\n",
      "[2,   238] loss: 0.275649\n",
      "[2,   239] accuracy: (48.000, 92.000)\n",
      "[2,   239] loss: 0.389709\n",
      "[2,   240] accuracy: (52.000, 92.000)\n",
      "[2,   240] loss: 0.350634\n",
      "[2,   241] accuracy: (48.000, 100.000)\n",
      "[2,   241] loss: 0.401048\n",
      "[2,   242] accuracy: (40.000, 100.000)\n",
      "[2,   242] loss: 0.347893\n",
      "[2,   243] accuracy: (56.000, 96.000)\n",
      "[2,   243] loss: 0.345457\n",
      "[2,   244] accuracy: (68.000, 96.000)\n",
      "[2,   244] loss: 0.252116\n",
      "[2,   245] accuracy: (44.000, 100.000)\n",
      "[2,   245] loss: 0.292887\n",
      "[2,   246] accuracy: (56.000, 96.000)\n",
      "[2,   246] loss: 0.262461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   247] accuracy: (48.000, 100.000)\n",
      "[2,   247] loss: 0.294170\n",
      "[2,   248] accuracy: (44.000, 92.000)\n",
      "[2,   248] loss: 0.413803\n",
      "[2,   249] accuracy: (36.000, 96.000)\n",
      "[2,   249] loss: 0.417565\n",
      "[2,   250] accuracy: (52.000, 96.000)\n",
      "[2,   250] loss: 0.315794\n",
      "[2,   251] accuracy: (48.000, 92.000)\n",
      "[2,   251] loss: 0.365734\n",
      "[2,   252] accuracy: (44.000, 96.000)\n",
      "[2,   252] loss: 0.358048\n",
      "[2,   253] accuracy: (60.000, 100.000)\n",
      "[2,   253] loss: 0.314237\n",
      "[2,   254] accuracy: (52.000, 100.000)\n",
      "[2,   254] loss: 0.332318\n",
      "[2,   255] accuracy: (52.000, 92.000)\n",
      "[2,   255] loss: 0.357087\n",
      "[2,   256] accuracy: (44.000, 96.000)\n",
      "[2,   256] loss: 0.332721\n",
      "[2,   257] accuracy: (52.000, 92.000)\n",
      "[2,   257] loss: 0.431098\n",
      "[2,   258] accuracy: (36.000, 92.000)\n",
      "[2,   258] loss: 0.361090\n",
      "[2,   259] accuracy: (44.000, 96.000)\n",
      "[2,   259] loss: 0.311500\n",
      "[2,   260] accuracy: (40.000, 96.000)\n",
      "[2,   260] loss: 0.386707\n",
      "[2,   261] accuracy: (40.000, 92.000)\n",
      "[2,   261] loss: 0.291294\n",
      "[2,   262] accuracy: (24.000, 96.000)\n",
      "[2,   262] loss: 0.347213\n",
      "[2,   263] accuracy: (48.000, 100.000)\n",
      "[2,   263] loss: 0.297624\n",
      "[2,   264] accuracy: (40.000, 96.000)\n",
      "[2,   264] loss: 0.381859\n",
      "[2,   265] accuracy: (44.000, 88.000)\n",
      "[2,   265] loss: 0.320233\n",
      "[2,   266] accuracy: (44.000, 88.000)\n",
      "[2,   266] loss: 0.390703\n",
      "[2,   267] accuracy: (52.000, 96.000)\n",
      "[2,   267] loss: 0.318884\n",
      "[2,   268] accuracy: (44.000, 100.000)\n",
      "[2,   268] loss: 0.364559\n",
      "[2,   269] accuracy: (40.000, 92.000)\n",
      "[2,   269] loss: 0.356427\n",
      "[2,   270] accuracy: (28.000, 92.000)\n",
      "[2,   270] loss: 0.380625\n",
      "[2,   271] accuracy: (32.000, 100.000)\n",
      "[2,   271] loss: 0.387218\n",
      "[2,   272] accuracy: (36.000, 100.000)\n",
      "[2,   272] loss: 0.333693\n",
      "[2,   273] accuracy: (28.000, 100.000)\n",
      "[2,   273] loss: 0.379075\n",
      "[2,   274] accuracy: (64.000, 100.000)\n",
      "[2,   274] loss: 0.252948\n",
      "[2,   275] accuracy: (44.000, 96.000)\n",
      "[2,   275] loss: 0.345004\n",
      "[2,   276] accuracy: (48.000, 96.000)\n",
      "[2,   276] loss: 0.328849\n",
      "[2,   277] accuracy: (36.000, 88.000)\n",
      "[2,   277] loss: 0.347538\n",
      "[2,   278] accuracy: (40.000, 100.000)\n",
      "[2,   278] loss: 0.374781\n",
      "[2,   279] accuracy: (44.000, 96.000)\n",
      "[2,   279] loss: 0.408604\n",
      "[2,   280] accuracy: (44.000, 96.000)\n",
      "[2,   280] loss: 0.314275\n",
      "[2,   281] accuracy: (56.000, 100.000)\n",
      "[2,   281] loss: 0.264355\n",
      "[2,   282] accuracy: (44.000, 92.000)\n",
      "[2,   282] loss: 0.330451\n",
      "[2,   283] accuracy: (48.000, 100.000)\n",
      "[2,   283] loss: 0.360361\n",
      "[2,   284] accuracy: (48.000, 96.000)\n",
      "[2,   284] loss: 0.325172\n",
      "[2,   285] accuracy: (56.000, 96.000)\n",
      "[2,   285] loss: 0.342488\n",
      "[2,   286] accuracy: (52.000, 96.000)\n",
      "[2,   286] loss: 0.339601\n",
      "[2,   287] accuracy: (32.000, 96.000)\n",
      "[2,   287] loss: 0.390609\n",
      "[2,   288] accuracy: (32.000, 96.000)\n",
      "[2,   288] loss: 0.331665\n",
      "[2,   289] accuracy: (56.000, 100.000)\n",
      "[2,   289] loss: 0.224746\n",
      "[2,   290] accuracy: (32.000, 96.000)\n",
      "[2,   290] loss: 0.384836\n",
      "[2,   291] accuracy: (32.000, 100.000)\n",
      "[2,   291] loss: 0.376778\n",
      "[2,   292] accuracy: (36.000, 100.000)\n",
      "[2,   292] loss: 0.376819\n",
      "[2,   293] accuracy: (36.000, 92.000)\n",
      "[2,   293] loss: 0.341900\n",
      "[2,   294] accuracy: (20.000, 100.000)\n",
      "[2,   294] loss: 0.337011\n",
      "[2,   295] accuracy: (44.000, 100.000)\n",
      "[2,   295] loss: 0.365805\n",
      "[2,   296] accuracy: (44.000, 96.000)\n",
      "[2,   296] loss: 0.326099\n",
      "[2,   297] accuracy: (56.000, 96.000)\n",
      "[2,   297] loss: 0.254353\n",
      "[2,   298] accuracy: (48.000, 100.000)\n",
      "[2,   298] loss: 0.303176\n",
      "[2,   299] accuracy: (56.000, 100.000)\n",
      "[2,   299] loss: 0.273944\n",
      "[2,   300] accuracy: (56.000, 100.000)\n",
      "[2,   300] loss: 0.286483\n",
      "[2,   301] accuracy: (28.000, 100.000)\n",
      "[2,   301] loss: 0.369284\n",
      "[2,   302] accuracy: (48.000, 96.000)\n",
      "[2,   302] loss: 0.291498\n",
      "[2,   303] accuracy: (44.000, 88.000)\n",
      "[2,   303] loss: 0.360312\n",
      "[2,   304] accuracy: (48.000, 92.000)\n",
      "[2,   304] loss: 0.348237\n",
      "[2,   305] accuracy: (44.000, 92.000)\n",
      "[2,   305] loss: 0.341352\n",
      "[2,   306] accuracy: (40.000, 92.000)\n",
      "[2,   306] loss: 0.412957\n",
      "[2,   307] accuracy: (48.000, 100.000)\n",
      "[2,   307] loss: 0.294898\n",
      "[2,   308] accuracy: (40.000, 96.000)\n",
      "[2,   308] loss: 0.355820\n",
      "[2,   309] accuracy: (28.000, 100.000)\n",
      "[2,   309] loss: 0.392549\n",
      "[2,   310] accuracy: (60.000, 100.000)\n",
      "[2,   310] loss: 0.280306\n",
      "[2,   311] accuracy: (56.000, 96.000)\n",
      "[2,   311] loss: 0.330848\n",
      "[2,   312] accuracy: (40.000, 100.000)\n",
      "[2,   312] loss: 0.262433\n",
      "[2,   313] accuracy: (36.000, 92.000)\n",
      "[2,   313] loss: 0.430790\n",
      "[2,   314] accuracy: (48.000, 100.000)\n",
      "[2,   314] loss: 0.356450\n",
      "[2,   315] accuracy: (24.000, 100.000)\n",
      "[2,   315] loss: 0.340284\n",
      "[2,   316] accuracy: (32.000, 88.000)\n",
      "[2,   316] loss: 0.385265\n",
      "[2,   317] accuracy: (32.000, 96.000)\n",
      "[2,   317] loss: 0.419349\n",
      "[2,   318] accuracy: (64.000, 96.000)\n",
      "[2,   318] loss: 0.283664\n",
      "[2,   319] accuracy: (36.000, 92.000)\n",
      "[2,   319] loss: 0.434395\n",
      "[2,   320] accuracy: (40.000, 92.000)\n",
      "[2,   320] loss: 0.373824\n",
      "[2,   321] accuracy: (48.000, 100.000)\n",
      "[2,   321] loss: 0.269952\n",
      "[2,   322] accuracy: (56.000, 96.000)\n",
      "[2,   322] loss: 0.322834\n",
      "[2,   323] accuracy: (48.000, 100.000)\n",
      "[2,   323] loss: 0.273372\n",
      "[2,   324] accuracy: (48.000, 92.000)\n",
      "[2,   324] loss: 0.391042\n",
      "[2,   325] accuracy: (56.000, 96.000)\n",
      "[2,   325] loss: 0.245085\n",
      "[2,   326] accuracy: (52.000, 100.000)\n",
      "[2,   326] loss: 0.311985\n",
      "[2,   327] accuracy: (24.000, 92.000)\n",
      "[2,   327] loss: 0.464888\n",
      "[2,   328] accuracy: (36.000, 100.000)\n",
      "[2,   328] loss: 0.310423\n",
      "[2,   329] accuracy: (56.000, 96.000)\n",
      "[2,   329] loss: 0.304227\n",
      "[2,   330] accuracy: (56.000, 100.000)\n",
      "[2,   330] loss: 0.367613\n",
      "[2,   331] accuracy: (52.000, 100.000)\n",
      "[2,   331] loss: 0.342832\n",
      "[2,   332] accuracy: (52.000, 88.000)\n",
      "[2,   332] loss: 0.305982\n",
      "[2,   333] accuracy: (56.000, 96.000)\n",
      "[2,   333] loss: 0.258493\n",
      "[2,   334] accuracy: (52.000, 84.000)\n",
      "[2,   334] loss: 0.331086\n",
      "[2,   335] accuracy: (48.000, 96.000)\n",
      "[2,   335] loss: 0.402542\n",
      "[2,   336] accuracy: (48.000, 96.000)\n",
      "[2,   336] loss: 0.303214\n",
      "[2,   337] accuracy: (40.000, 100.000)\n",
      "[2,   337] loss: 0.284011\n",
      "[2,   338] accuracy: (52.000, 100.000)\n",
      "[2,   338] loss: 0.279411\n",
      "[2,   339] accuracy: (68.000, 88.000)\n",
      "[2,   339] loss: 0.383182\n",
      "[2,   340] accuracy: (40.000, 100.000)\n",
      "[2,   340] loss: 0.278536\n",
      "[2,   341] accuracy: (52.000, 100.000)\n",
      "[2,   341] loss: 0.346849\n",
      "[2,   342] accuracy: (48.000, 96.000)\n",
      "[2,   342] loss: 0.308707\n",
      "[2,   343] accuracy: (28.000, 96.000)\n",
      "[2,   343] loss: 0.406093\n",
      "[2,   344] accuracy: (36.000, 100.000)\n",
      "[2,   344] loss: 0.322903\n",
      "[2,   345] accuracy: (40.000, 88.000)\n",
      "[2,   345] loss: 0.349616\n",
      "[2,   346] accuracy: (32.000, 100.000)\n",
      "[2,   346] loss: 0.382570\n",
      "[2,   347] accuracy: (64.000, 100.000)\n",
      "[2,   347] loss: 0.257110\n",
      "[2,   348] accuracy: (40.000, 88.000)\n",
      "[2,   348] loss: 0.316411\n",
      "[2,   349] accuracy: (52.000, 100.000)\n",
      "[2,   349] loss: 0.340631\n",
      "[2,   350] accuracy: (64.000, 96.000)\n",
      "[2,   350] loss: 0.251880\n",
      "[2,   351] accuracy: (40.000, 96.000)\n",
      "[2,   351] loss: 0.394545\n",
      "[2,   352] accuracy: (40.000, 96.000)\n",
      "[2,   352] loss: 0.404183\n",
      "[2,   353] accuracy: (48.000, 100.000)\n",
      "[2,   353] loss: 0.285407\n",
      "[2,   354] accuracy: (56.000, 100.000)\n",
      "[2,   354] loss: 0.365084\n",
      "[2,   355] accuracy: (36.000, 96.000)\n",
      "[2,   355] loss: 0.328222\n",
      "[2,   356] accuracy: (36.000, 100.000)\n",
      "[2,   356] loss: 0.302998\n",
      "[2,   357] accuracy: (32.000, 96.000)\n",
      "[2,   357] loss: 0.358619\n",
      "[2,   358] accuracy: (32.000, 88.000)\n",
      "[2,   358] loss: 0.406138\n",
      "[2,   359] accuracy: (52.000, 96.000)\n",
      "[2,   359] loss: 0.298597\n",
      "[2,   360] accuracy: (48.000, 84.000)\n",
      "[2,   360] loss: 0.333146\n",
      "[2,   361] accuracy: (48.000, 96.000)\n",
      "[2,   361] loss: 0.327406\n",
      "[2,   362] accuracy: (40.000, 96.000)\n",
      "[2,   362] loss: 0.300639\n",
      "[2,   363] accuracy: (52.000, 100.000)\n",
      "[2,   363] loss: 0.269429\n",
      "[2,   364] accuracy: (40.000, 96.000)\n",
      "[2,   364] loss: 0.375504\n",
      "[2,   365] accuracy: (56.000, 92.000)\n",
      "[2,   365] loss: 0.316086\n",
      "[2,   366] accuracy: (40.000, 100.000)\n",
      "[2,   366] loss: 0.300547\n",
      "[2,   367] accuracy: (40.000, 88.000)\n",
      "[2,   367] loss: 0.300597\n",
      "[2,   368] accuracy: (48.000, 88.000)\n",
      "[2,   368] loss: 0.349505\n",
      "[2,   369] accuracy: (44.000, 92.000)\n",
      "[2,   369] loss: 0.357808\n",
      "[2,   370] accuracy: (60.000, 96.000)\n",
      "[2,   370] loss: 0.274770\n",
      "[2,   371] accuracy: (40.000, 100.000)\n",
      "[2,   371] loss: 0.319544\n",
      "[2,   372] accuracy: (40.000, 100.000)\n",
      "[2,   372] loss: 0.301689\n",
      "[2,   373] accuracy: (40.000, 92.000)\n",
      "[2,   373] loss: 0.359295\n",
      "[2,   374] accuracy: (52.000, 100.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2,   374] loss: 0.282248\n",
      "[2,   375] accuracy: (44.000, 96.000)\n",
      "[2,   375] loss: 0.324487\n",
      "[2,   376] accuracy: (48.000, 96.000)\n",
      "[2,   376] loss: 0.311645\n",
      "[2,   377] accuracy: (52.000, 96.000)\n",
      "[2,   377] loss: 0.346868\n",
      "[2,   378] accuracy: (48.000, 96.000)\n",
      "[2,   378] loss: 0.340029\n",
      "[2,   379] accuracy: (40.000, 96.000)\n",
      "[2,   379] loss: 0.448244\n",
      "[2,   380] accuracy: (28.000, 96.000)\n",
      "[2,   380] loss: 0.379089\n",
      "[2,   381] accuracy: (48.000, 100.000)\n",
      "[2,   381] loss: 0.295918\n",
      "[2,   382] accuracy: (40.000, 100.000)\n",
      "[2,   382] loss: 0.377850\n",
      "[2,   383] accuracy: (56.000, 92.000)\n",
      "[2,   383] loss: 0.361118\n",
      "[2,   384] accuracy: (52.000, 96.000)\n",
      "[2,   384] loss: 0.357928\n",
      "[2,   385] accuracy: (52.000, 92.000)\n",
      "[2,   385] loss: 0.387262\n",
      "[2,   386] accuracy: (48.000, 96.000)\n",
      "[2,   386] loss: 0.366256\n",
      "[2,   387] accuracy: (44.000, 100.000)\n",
      "[2,   387] loss: 0.315021\n",
      "[2,   388] accuracy: (28.000, 92.000)\n",
      "[2,   388] loss: 0.518656\n",
      "[2,   389] accuracy: (24.000, 92.000)\n",
      "[2,   389] loss: 0.457699\n",
      "[2,   390] accuracy: (48.000, 96.000)\n",
      "[2,   390] loss: 0.365470\n",
      "[2,   391] accuracy: (37.500, 100.000)\n",
      "[2,   391] loss: 0.300461\n",
      "COMPLETED EPOCH 0002... checkpointing here\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_loss = nn.CrossEntropyLoss() # just use standard XEntropy to train\n",
    "training_logger = training_obj.train(cifar_trainset, 2, train_loss, \n",
    "                                     attack_parameters=attack_params, \n",
    "                                     verbosity='snoop', loglevel='snoop') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printouts look like:\n",
    "``` \n",
    "[epoch_no, minibatch_no] accuracy: (X, Y) \n",
    "[epoch_no, minibatch_no] loss: Z\n",
    "```\n",
    "\n",
    "- X is the percent of successfully classified *adversarial* examples generated from that minibatch only\n",
    "- Y is the percent of successfully classified *original* examples on that minibatch only\n",
    "- Z is the value of the loss function after that minibatch\n",
    "\n",
    "The output of training is a `TrainingLogger` class that stores essentially what was printed, and is separately controlled by its own `loglevel` argument. This can be accessed (and just to be safe, sorted using `sort_series`) and plotted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c2778c630>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt \n",
    "plt.plot(training_logger.sort_series('training_loss', return_keys=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e650080>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot adversarial accuracy as we train \n",
    "# This should be noisy, but generally going UP\n",
    "plt.plot([_[0] for _ in training_logger.sort_series('attack', return_keys=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once training completes, you can verify that the checkpoints are indeed stored in wherever you have set up pretrained models to be stored. By default this is `mister_ed/pretrained_models/`, so you should have a `tutorial_fgsm.resnet20.000002.path.tar` file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restarting from Checkpoint\n",
    "When training, sometimes @#\\$& happens and things break. This is why we checkpoint. Here we'll show how to restart from checkpoint in training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to pick back up from where we left off with the experiment/architecture pair defined above `(tutorial_fgsm, resnet20)`. Then we want to do the following steps:\n",
    "\n",
    "1. Instantiate a model of the same architecture (weights don't matter, since we'll load from the checkpoint) \n",
    "2. Build an `AdversarialTraining` object using this model, its normalizer, and the same experiment name, architecture name \n",
    "3. Build a loss function, attack_parameters object, and all other identical kwargs from the first (aborted) training run \n",
    "4. Run the training using the training object's `train_from_checkpoint` method instead of `train`. All the kwargs are the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3,   100] accuracy: (52.000, 100.000)\n",
      "[3,   100] loss: 0.345297\n",
      "[3,   200] accuracy: (44.000, 88.000)\n",
      "[3,   200] loss: 0.326587\n",
      "[3,   300] accuracy: (52.000, 92.000)\n",
      "[3,   300] loss: 0.336156\n",
      "COMPLETED EPOCH 0003... checkpointing here\n",
      "[4,   100] accuracy: (64.000, 100.000)\n",
      "[4,   100] loss: 0.312935\n",
      "[4,   200] accuracy: (52.000, 84.000)\n",
      "[4,   200] loss: 0.325036\n",
      "[4,   300] accuracy: (44.000, 96.000)\n",
      "[4,   300] loss: 0.333294\n",
      "COMPLETED EPOCH 0004... checkpointing here\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "naive_model, normalizer = cifar_loader.load_pretrained_cifar_resnet(flavor=20, return_normalizer=True)\n",
    "new_train_obj = advtrain.AdversarialTraining(naive_model, normalizer, experiment_name, architecture)\n",
    "\n",
    "delta_threat = ap.ThreatModel(ap.DeltaAddition, \n",
    "                              {'lp_style': 'inf', \n",
    "                               'lp_bound': 8.0 / 255})\n",
    "attack_loss = plf.VanillaXentropy(naive_model, normalizer)\n",
    "attack_object = aa.FGSM(naive_model, normalizer, delta_threat, attack_loss)\n",
    "attack_kwargs = {'verbose': False} # kwargs to be called in attack_object.attack(...)\n",
    "attack_params = advtrain.AdversarialAttackParameters(attack_object, proportion_attacked=0.2, \n",
    "                                                     attack_specific_params={'attack_kwargs': attack_kwargs})\n",
    "\n",
    "new_train_obj.train_from_checkpoint(cifar_trainset, 4, train_loss, attack_parameters=attack_params, \n",
    "                                    verbosity='high')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this finishes, notice that you should now have a file \n",
    "`tutorial_fgsm.resnet20.000004.path.tar` in your pretrained_models directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the training script \n",
    "Using an ipython notebook isn't typically ideal for training, since it mandates you keep your browser window open. To this end, we've built a script to perform adversarial training in a tmux/screen background. This is located in `scripts/advtrain.py`. Here's what we've found are best practices for doing this:\n",
    "\n",
    "- Copy `scripts/advtrain.py` into `scripts/advtrain_<DESCRIPTIVE_EXPERIMENT_NAME>.py`\n",
    "- Modify the `build_attack_params` method in `scripts/advtrain_<DESCRIPTIVE_EXPERIMENT_NAME>.py` to use the attack parameters that you want. There's plenty of prebuilt attack parameters in that file to choose from. \n",
    "- In a tmux/screen, from `mister_ed`, run \n",
    "\n",
    "```python -m scripts.advtrain_DESCRIPTIVE_EXPERIMENT_NAME --exp <DESCRIPTIVE_EXPERIMENT_NAME> --arch <ARCHITECTURE_CHOICE> --verbosity [snoop/high/medium]```\n",
    "\n",
    "- To resume, you can optionally add the `-r` or `--resume` flag to the script call\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A note on GPU Usage\n",
    "If you have access to a GPU on your machine, you'll probably want to leverage its power when doing training and attacks. `mister_ed` has been designed so \"standard\" GPU behavior should be supported without any extra effort. By \"standard\" GPU behavior, I mean that all objects reside on the same device: either all on the GPU or none on the GPU. If there is a GPU on your machine, which one can check from the output of \n",
    "```\n",
    "import torch.cuda as cuda \n",
    "print(cuda.is_available()) \n",
    "```\n",
    "Globally, unless otherwise specified, all objects will be initialized in GPU-mode if this output is `True`. This is done behind-the-scenes by setting the environment variable `MISTER_ED_GPU`. If you have access to a GPU, but wouldn't like to use it, you can manually override this environment variable by calling:\n",
    "```\n",
    "import utils.pytorch_utils as utils \n",
    "utils.set_global_gpu(False)\n",
    "```\n",
    "And then none of your objects will be in GPU-mode by default. \n",
    "\n",
    "For nonstandard GPU behavior, you should initialize any object that differs from the default gpu status (as defined by `MISTER_ED_GPU`) with the kwarg `manual_gpu=<True/False>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
